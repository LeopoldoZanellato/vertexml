{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab029592-0d8d-4ae0-bee1-8ac18ff2bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import json\n",
    "from typing import NamedTuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import component\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Output, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74091d5f-7271-4641-8cf2-62b7c60b9ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARIABLES\n",
    "PROJECT_ID = \"vertexai-457414\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://the_chustomer_churn/dataset/\"  # @param {type:\"string\"}\n",
    "PIPELINES_PATH = f\"{BUCKET_URI}pipelines/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08fca3ee-8f05-4464-af66-b08a5301656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, \n",
    "                location=LOCATION, \n",
    "                staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d51c2d9e-b3c5-4267-9814-69d135f8bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\",\n",
    "              packages_to_install=[\n",
    "                \"pandas\",\n",
    "                \"scikit-learn\",\n",
    "                \"fsspec\",\n",
    "                \"gcsfs\" \n",
    "            ])\n",
    "def load_data(data_raw_path: str,\n",
    "              df_train: Output[Dataset],\n",
    "              df_test: Output[Dataset],\n",
    "              df_val: Output[Dataset]):\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # VARIABLES\n",
    "    TEST_SIZE = 0.15  # 15% para teste\n",
    "    VAL_SIZE = 0.15   # 15% para validação\n",
    "    TARGET_COLUMN = 'Churn'\n",
    "\n",
    "    # LEITURA\n",
    "    dataset = pd.read_csv(f\"gs://the_chustomer_churn/dataset/dataset.csv\")\n",
    "\n",
    "\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        dataset,\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=42,\n",
    "        stratify=dataset[TARGET_COLUMN]\n",
    "    )\n",
    "\n",
    "\n",
    "    # Etapa 2: Separar o restante (85%) em Treino (70%) e Validação (15%)\n",
    "    # Precisamos recalcular a proporção de validação em relação ao 'train_val_df'\n",
    "    # val_size_ratio = 15% / 85%\n",
    "    val_size_ratio = VAL_SIZE / (1 - TEST_SIZE)\n",
    "    \n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df,\n",
    "        test_size=val_size_ratio,\n",
    "        random_state=42,\n",
    "        stratify=train_val_df[TARGET_COLUMN] # Estratificar novamente\n",
    "    )\n",
    "\n",
    "\n",
    "    train_df.to_csv(df_train.path, index=False)\n",
    "    test_df.to_csv(df_test.path, index=False)\n",
    "    val_df.to_csv(df_val.path, index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85ec4a56-c1cd-43ae-8abc-489601816237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl import (\n",
    "    Input, \n",
    "    Model, \n",
    "    Metrics, \n",
    "    component\n",
    ")\n",
    "@component(base_image=\"python:3.9\",\n",
    "              packages_to_install=[\n",
    "                \"google-cloud-aiplatform\", \n",
    "                \"pandas\",\n",
    "                \"scikit-learn\",\n",
    "                \"fsspec\",\n",
    "                \"gcsfs\" \n",
    "            ])\n",
    "def preprocess_features(\n",
    "              df_train_csv: Input[Dataset],\n",
    "              df_test_csv: Input[Dataset],\n",
    "              df_val_csv: Input[Dataset],\n",
    "              df_train_preprocessed: Output[Dataset],\n",
    "              df_test_preprocessed: Output[Dataset],\n",
    "              df_val_preprocessed: Output[Dataset]\n",
    "              ):\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    def preprocess_features(df_input):\n",
    "        \"\"\"\n",
    "        Função de LIMPEZA: Corrige tipos, trata nulos e mapeia o alvo.\n",
    "        Não faz 'fit' ou 'transform' (scaling ou encoding).\n",
    "        \"\"\"\n",
    "        # 1. Copiar o dataframe\n",
    "        df = df_input.copy()\n",
    "        \n",
    "        # --- ETAPA 1: Correção de Tipos e Nulos ---\n",
    "        print(f\"Limpando TotalCharges...\")\n",
    "        df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "        df['TotalCharges'].fillna(0, inplace=True)\n",
    "        \n",
    "        \n",
    "        # --- ETAPA 2: Mapeamento APENAS DA COLUNA-ALVO ---\n",
    "        # O 'y' (target) precisa ser uma coluna única (0 ou 1)\n",
    "        if 'Churn' in df.columns:\n",
    "            print(f\"Mapeando coluna 'Churn' para 0/1...\")\n",
    "            df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "            \n",
    "        # --- ETAPA 3: Preparação das Categóricas ---\n",
    "        # Apenas garante que SeniorCitizen seja 'str' para o OneHotEncoder\n",
    "        if 'SeniorCitizen' in df.columns:\n",
    "            df['SeniorCitizen'] = df['SeniorCitizen'].astype(str)\n",
    "            \n",
    "        \n",
    "        # --- ETAPA 4: Remoção de Colunas ---\n",
    "        if 'customerID' in df.columns:\n",
    "            df = df.drop(columns=['customerID'])\n",
    "            \n",
    "        return df\n",
    "\n",
    "\n",
    "    \n",
    "    df_train_raw = pd.read_csv(df_train_csv.path)\n",
    "    df_test_raw = pd.read_csv(df_test_csv.path)\n",
    "    df_val_raw = pd.read_csv(df_test_csv.path)\n",
    "\n",
    "\n",
    "    df_train = preprocess_features(df_train_raw)\n",
    "    \n",
    "    print(\"Processando dataset de VALIDAÇÃO...\")\n",
    "    df_val = preprocess_features(df_val_raw)\n",
    "    \n",
    "    print(\"Processando dataset de TESTE...\")\n",
    "    df_test = preprocess_features(df_test_raw)\n",
    "        \n",
    "    df_train.to_csv(df_train_preprocessed.path, index=False)\n",
    "    df_test.to_csv(df_test_preprocessed.path, index=False)\n",
    "    df_val.to_csv(df_val_preprocessed.path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc93ed76-ff56-4a92-9997-8fdb0cc2b282",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\",\n",
    "              packages_to_install=[\n",
    "                \"google-cloud-aiplatform\", \n",
    "                \"pandas\",\n",
    "                \"scikit-learn\",\n",
    "                \"fsspec\",\n",
    "                \"gcsfs\" \n",
    "            ])\n",
    "\n",
    "\n",
    "\n",
    "def transformers(\n",
    "              # Entradas\n",
    "              df_train_csv: Input[Dataset],\n",
    "              df_test_csv: Input[Dataset],\n",
    "              df_val_csv: Input[Dataset],\n",
    "\n",
    "              # Saídas\n",
    "              y_train_out: Output[Dataset],\n",
    "              X_train_out: Output[Dataset],\n",
    "              y_val_out: Output[Dataset],\n",
    "              X_val_out: Output[Dataset],\n",
    "              y_test_out: Output[Dataset],\n",
    "              X_test_out: Output[Dataset],\n",
    "\n",
    "              # Artifatos\n",
    "              preprocessor_artifact: Output[Model]\n",
    "              ):\n",
    "\n",
    "    from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    import pandas as pd\n",
    "\n",
    "    \n",
    "    df_train = pd.read_csv(df_train_csv.path)\n",
    "    df_test = pd.read_csv(df_test_csv.path)\n",
    "    df_val = pd.read_csv(df_test_csv.path)\n",
    "\n",
    "\n",
    "    y_train = df_train.pop('Churn')\n",
    "    y_val = df_val.pop('Churn')\n",
    "    y_test = df_test.pop('Churn')\n",
    "    \n",
    "    # O que sobra em df_train, df_val, df_test é o nosso X\n",
    "    X_train = df_train\n",
    "    X_val = df_val\n",
    "    X_test = df_test\n",
    "\n",
    "\n",
    "\n",
    "    # --- 3. Definir Colunas Numéricas e Categóricas (para o Pipeline) ---\n",
    "\n",
    "    numeric_features = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "    \n",
    "    categorical_features = [\n",
    "        'gender', 'SeniorCitizen', 'Partner', 'Dependents', \n",
    "        'PhoneService', 'MultipleLines', 'InternetService', \n",
    "        'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', \n",
    "        'TechSupport', 'StreamingTV', 'StreamingMovies', \n",
    "        'Contract', 'PaperlessBilling', 'PaymentMethod'\n",
    "    ]\n",
    "    \n",
    "    # --- 4. Criar o Preprocessor (O \"Cérebro\") ---\n",
    "    \n",
    "    # Pipeline para features numéricas\n",
    "    numeric_transformer = MinMaxScaler()\n",
    "    \n",
    "    # Pipeline para features categóricas\n",
    "    categorical_transformer = OneHotEncoder(\n",
    "        drop='first',              \n",
    "        handle_unknown='ignore',   # Ignora categorias raras que apareçam no teste\n",
    "        sparse_output=False        \n",
    "    )\n",
    "    \n",
    "    # Juntar tudo no ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough' \n",
    "    )\n",
    "    \n",
    "    \n",
    "    # --- 5. FIT (Aprender) e TRANSFORM (Aplicar) ---\n",
    "    \n",
    "    # FIT (APRENDER) - SOMENTE COM X_train\n",
    "    print(\"\\n'Fitando' (aprendendo) o preprocessor com X_train...\")\n",
    "    preprocessor.fit(X_train)\n",
    "    \n",
    "    # TRANSFORM (APLICAR) - Em todos os datasets\n",
    "    print(\"Transformando X_train...\")\n",
    "    X_train_processed = preprocessor.transform(X_train)\n",
    "    \n",
    "    print(\"Transformando X_val...\")\n",
    "    X_val_processed = preprocessor.transform(X_val)\n",
    "    \n",
    "    print(\"Transformando X_test...\")\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    print(\"\\n✅ Processamento com Pipeline concluído com sucesso!\")\n",
    "    print(f\"Formato do X_train processado: {X_train_processed.shape}\")\n",
    "    print(f\"Formato do X_val processado: {X_val_processed.shape}\")\n",
    "    print(f\"Formato do X_test processado: {X_test_processed.shape}\")\n",
    "\n",
    "\n",
    "    # --- 4. SALVAR OS DADOS PROCESSADOS NAS SAÍDAS ---\n",
    "    print(\"Salvando dados processados em CSV...\")\n",
    "    try:\n",
    "        col_names = preprocessor.get_feature_names_out()\n",
    "    except Exception:\n",
    "        # Fallback simples se get_feature_names_out falhar\n",
    "        col_names = [f\"feature_{i}\" for i in range(X_train_processed.shape[1])]\n",
    "\n",
    "    pd.DataFrame(X_train_processed, columns=col_names).to_csv(X_train_out.path, index=False)\n",
    "    pd.DataFrame(X_val_processed, columns=col_names).to_csv(X_val_out.path, index=False)\n",
    "    pd.DataFrame(X_test_processed, columns=col_names).to_csv(X_test_out.path, index=False)\n",
    "    \n",
    "    y_train.to_csv(y_train_out.path, index=False)\n",
    "    y_val.to_csv(y_val_out.path, index=False)\n",
    "    y_test.to_csv(y_test_out.path, index=False)\n",
    "\n",
    "    print(\"\\n✅ Processamento e salvamento concluídos!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd3c762f-92a8-4abb-8c06-1495dbfd0f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl import component, Input, Output, Model, Dataset\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        \"pandas\", \"scikit-learn\", \"joblib\", \"xgboost\", \"fsspec\", \"gcsfs\"\n",
    "    ],\n",
    ")\n",
    "def train_model(\n",
    "    x_train_processed: Input[Dataset],\n",
    "    y_train: Input[Dataset],\n",
    "    x_val_processed: Input[Dataset],\n",
    "    y_val: Input[Dataset],\n",
    "    scale_pos_weight: float,\n",
    "    n_estimators: int,\n",
    "    random_state: int,\n",
    "    eval_metric: str,\n",
    "    objective: str,\n",
    "    trained_model: Output[Model],\n",
    "):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import xgboost as xgb\n",
    "\n",
    "\n",
    "\n",
    "    # --- ler dados ---\n",
    "    X_train = pd.read_csv(x_train_processed.path)\n",
    "    y_train_s = pd.read_csv(y_train.path)\n",
    "    X_val = pd.read_csv(x_val_processed.path)\n",
    "    y_val_s = pd.read_csv(y_val.path)\n",
    "\n",
    "    # --- modelo ---\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective=objective,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=random_state,\n",
    "        n_estimators=n_estimators,\n",
    "        eval_metric=eval_metric,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\",\n",
    "    )\n",
    "\n",
    "    # --- treino (sem logs/plots) ---\n",
    "    model.fit(X_train, y_train_s, eval_set=[(X_val, y_val_s)], verbose=False)\n",
    "\n",
    "    # --- salvar ---\n",
    "    #joblib.dump(model, trained_model.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01209d70-1d5e-47aa-806f-644308996a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"customer\",\n",
    "    description=\"Pipeline from customer dataset\",\n",
    "    pipeline_root=PIPELINES_PATH,\n",
    ")\n",
    "def pipeline(\n",
    "    # Parâmetros de entrada do Pipeline\n",
    "    data_raw_path: str = \"gs://the_chustomer_churn/dataset/dataset.csv\",\n",
    "    \n",
    "    # Hiperparâmetros para o treinamento\n",
    "    hp_scale_pos_weight: float = 3.0,  # Valor padrão, você pode mudar na UI\n",
    "    hp_n_estimators: int = 150,\n",
    "    hp_random_state: int = 42\n",
    "):\n",
    "    \n",
    "    # 1. Carrega e divide os dados\n",
    "    load_task = load_data(data_raw_path=data_raw_path)\n",
    "    \n",
    "    # 2. Faz a limpeza (ex: TotalCharges)\n",
    "    preprocess_task = preprocess_features(\n",
    "        df_train_csv = load_task.outputs[\"df_train\"],\n",
    "        df_test_csv = load_task.outputs['df_test'],\n",
    "        df_val_csv = load_task.outputs['df_val']\n",
    "    )\n",
    "\n",
    "    # 3. Aplica o ColumnTransformer (MinMaxScaler, OneHotEncoder)\n",
    "    transformer_task = transformers(\n",
    "        df_train_csv = preprocess_task.outputs['df_train_preprocessed'],\n",
    "        df_test_csv = preprocess_task.outputs['df_test_preprocessed'],\n",
    "        df_val_csv = preprocess_task.outputs['df_val_preprocessed']\n",
    "    )\n",
    "\n",
    "    # --- AJUSTE AQUI ---\n",
    "    # 4. Treina o modelo com os dados transformados\n",
    "    train_task = train_model(\n",
    "        # Dados vêm do 'transformer_task'\n",
    "        x_train_processed=transformer_task.outputs[\"X_train_out\"],\n",
    "        y_train=transformer_task.outputs[\"y_train_out\"],\n",
    "        x_val_processed=transformer_task.outputs[\"X_val_out\"],\n",
    "        y_val=transformer_task.outputs[\"y_val_out\"],\n",
    "        \n",
    "        # Hiperparâmetros vêm da definição do pipeline\n",
    "        scale_pos_weight=hp_scale_pos_weight,\n",
    "        n_estimators=hp_n_estimators,\n",
    "        random_state=hp_random_state,\n",
    "        \n",
    "        # Parâmetros fixos\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58f8876e-56d2-46c1-827d-454efd9b8b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24bb2bae-f98d-46bd-a8a7-9f35754edad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/666534829864/locations/us-central1/pipelineJobs/customer-20251026224156\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/666534829864/locations/us-central1/pipelineJobs/customer-20251026224156')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/customer-20251026224156?project=666534829864\n",
      "PipelineJob projects/666534829864/locations/us-central1/pipelineJobs/customer-20251026224156 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/666534829864/locations/us-central1/pipelineJobs/customer-20251026224156 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/666534829864/locations/us-central1/pipelineJobs/customer-20251026224156 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/666534829864/locations/us-central1/pipelineJobs/customer-20251026224156 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/666534829864/locations/us-central1/pipelineJobs/customer-20251026224156 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/666534829864/locations/us-central1/pipelineJobs/customer-20251026224156\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(project=\"vertexai-457414\", location=\"us-central1\")\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"custumer-dataset-v1\",\n",
    "    template_path=\"pipeline.json\",\n",
    "    pipeline_root=\"gs://the_chustomer_churn/dataset/pipelines/\"\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2128641b-aeef-474f-901d-1b29bf7423b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f13c44-9e45-4e6e-94ba-711e11383147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0ea661-bb97-4570-ab11-76e4c5957c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vertexml)",
   "language": "python",
   "name": "vertexml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
